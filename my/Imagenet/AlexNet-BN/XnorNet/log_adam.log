I0111 22:36:28.224843 32596 upgrade_proto.cpp:1113] snapshot_prefix was a directory and is replaced to snapshot/solver_adam
I0111 22:36:28.225653 32596 caffe.cpp:204] Using GPUs 3
I0111 22:36:28.355072 32596 caffe.cpp:209] GPU 3: GeForce GTX 1080 Ti
I0111 22:36:29.925657 32596 solver.cpp:45] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
display: 200
max_iter: 500000
lr_policy: "modified_lr"
momentum: 0.9
snapshot: 10000
snapshot_prefix: "snapshot/solver_adam"
solver_mode: GPU
device_id: 3
net: "train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
momentum2: 0.999
type: "Adam"
modified_lr {
  stepvalue: 120000
  stepvalue: 260000
  stepvalue: 370000
  stepvalue: 500000
  mlr: 0.001
  mlr: 0.0001
  mlr: 1e-05
  mlr: 1e-06
  weight_decay: 0
  weight_decay: 0
  weight_decay: 0
  weight_decay: 0
}
I0111 22:36:29.927587 32596 solver.cpp:105] Creating training net from net file: train_test.prototxt
I0111 22:36:29.928762 32596 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0111 22:36:29.929000 32596 net.cpp:51] Initializing net from parameters: 
name: "AlexNet-BN"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/home/zhengzhe/Data/imagenet_shrt256/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "pool1"
  top: "pool1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive1"
  type: "BinActive"
  bottom: "pool1"
  top: "binactive1"
}
layer {
  name: "conv2"
  type: "BinaryConvolution"
  bottom: "binactive1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "pool2"
  top: "pool2"
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "pool2"
  top: "pool2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive2"
  type: "BinActive"
  bottom: "pool2"
  top: "binactive2"
}
layer {
  name: "conv3"
  type: "BinaryConvolution"
  bottom: "binactive2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive3"
  type: "BinActive"
  bottom: "conv3"
  top: "binactive3"
}
layer {
  name: "conv4"
  type: "BinaryConvolution"
  bottom: "binactive3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive4"
  type: "BinActive"
  bottom: "conv4"
  top: "binactive4"
}
layer {
  name: "conv5"
  type: "BinaryConvolution"
  bottom: "binactive4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "bn6"
  type: "BatchNorm"
  bottom: "pool5"
  top: "pool5"
}
layer {
  name: "scale6"
  type: "Scale"
  bottom: "pool5"
  top: "pool5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive5"
  type: "BinActive"
  bottom: "pool5"
  top: "binactive5"
}
layer {
  name: "fc6"
  type: "BinaryInnerProduct"
  bottom: "binactive5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4096
    bias_term: false
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "scale7"
  type: "Scale"
  bottom: "fc6"
  top: "fc6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive6"
  type: "BinActive"
  bottom: "fc6"
  top: "binactive6"
}
layer {
  name: "fc7"
  type: "BinaryInnerProduct"
  bottom: "binactive6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4096
    bias_term: false
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn8"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "scale8"
  type: "Scale"
  bottom: "fc7"
  top: "fc7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
I0111 22:36:29.929924 32596 layer_factory.hpp:78] Creating layer data
I0111 22:36:29.930248 32596 db_lmdb.cpp:35] Opened lmdb /home/zhengzhe/Data/imagenet_shrt256/ilsvrc12_train_lmdb
I0111 22:36:29.930385 32596 net.cpp:84] Creating Layer data
I0111 22:36:29.930445 32596 net.cpp:380] data -> data
I0111 22:36:29.930627 32596 net.cpp:380] data -> label
I0111 22:36:29.932719 32596 data_layer.cpp:45] output data size: 256,3,224,224
I0111 22:36:30.346761 32596 base_data_layer.cpp:72] Initializing prefetch
I0111 22:36:30.895722 32596 base_data_layer.cpp:75] Prefetch initialized.
I0111 22:36:30.895768 32596 net.cpp:122] Setting up data
I0111 22:36:30.895819 32596 net.cpp:129] Top shape: 256 3 224 224 (38535168)
I0111 22:36:30.895830 32596 net.cpp:129] Top shape: 256 (256)
I0111 22:36:30.895833 32596 net.cpp:137] Memory required for data: 154141696
I0111 22:36:30.895876 32596 layer_factory.hpp:78] Creating layer label_data_1_split
I0111 22:36:30.895948 32596 net.cpp:84] Creating Layer label_data_1_split
I0111 22:36:30.895977 32596 net.cpp:406] label_data_1_split <- label
I0111 22:36:30.896039 32596 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0111 22:36:30.896082 32596 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0111 22:36:30.896100 32596 net.cpp:380] label_data_1_split -> label_data_1_split_2
I0111 22:36:30.896253 32596 net.cpp:122] Setting up label_data_1_split
I0111 22:36:30.896267 32596 net.cpp:129] Top shape: 256 (256)
I0111 22:36:30.896282 32596 net.cpp:129] Top shape: 256 (256)
I0111 22:36:30.896288 32596 net.cpp:129] Top shape: 256 (256)
I0111 22:36:30.896292 32596 net.cpp:137] Memory required for data: 154144768
I0111 22:36:30.896297 32596 layer_factory.hpp:78] Creating layer conv1
I0111 22:36:30.896414 32596 net.cpp:84] Creating Layer conv1
I0111 22:36:30.896432 32596 net.cpp:406] conv1 <- data
I0111 22:36:30.896492 32596 net.cpp:380] conv1 -> conv1
I0111 22:36:32.138854 32596 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 127416
I0111 22:36:32.139191 32596 net.cpp:122] Setting up conv1
I0111 22:36:32.139219 32596 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0111 22:36:32.139225 32596 net.cpp:137] Memory required for data: 451514368
I0111 22:36:32.139320 32596 layer_factory.hpp:78] Creating layer bn1
I0111 22:36:32.139372 32596 net.cpp:84] Creating Layer bn1
I0111 22:36:32.139387 32596 net.cpp:406] bn1 <- conv1
I0111 22:36:32.139418 32596 net.cpp:367] bn1 -> conv1 (in-place)
I0111 22:36:32.140992 32596 net.cpp:122] Setting up bn1
I0111 22:36:32.141010 32596 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0111 22:36:32.141016 32596 net.cpp:137] Memory required for data: 748883968
I0111 22:36:32.141062 32596 layer_factory.hpp:78] Creating layer scale1
I0111 22:36:32.141100 32596 net.cpp:84] Creating Layer scale1
I0111 22:36:32.141111 32596 net.cpp:406] scale1 <- conv1
I0111 22:36:32.141129 32596 net.cpp:367] scale1 -> conv1 (in-place)
I0111 22:36:32.141224 32596 layer_factory.hpp:78] Creating layer scale1
I0111 22:36:32.141453 32596 net.cpp:122] Setting up scale1
I0111 22:36:32.141469 32596 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0111 22:36:32.141474 32596 net.cpp:137] Memory required for data: 1046253568
I0111 22:36:32.141499 32596 layer_factory.hpp:78] Creating layer relu1
I0111 22:36:32.141530 32596 net.cpp:84] Creating Layer relu1
I0111 22:36:32.141538 32596 net.cpp:406] relu1 <- conv1
I0111 22:36:32.141553 32596 net.cpp:367] relu1 -> conv1 (in-place)
I0111 22:36:32.142136 32596 net.cpp:122] Setting up relu1
I0111 22:36:32.142153 32596 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0111 22:36:32.142156 32596 net.cpp:137] Memory required for data: 1343623168
I0111 22:36:32.142163 32596 layer_factory.hpp:78] Creating layer pool1
I0111 22:36:32.142189 32596 net.cpp:84] Creating Layer pool1
I0111 22:36:32.142199 32596 net.cpp:406] pool1 <- conv1
I0111 22:36:32.142216 32596 net.cpp:380] pool1 -> pool1
I0111 22:36:32.142305 32596 net.cpp:122] Setting up pool1
I0111 22:36:32.142318 32596 net.cpp:129] Top shape: 256 96 27 27 (17915904)
I0111 22:36:32.142323 32596 net.cpp:137] Memory required for data: 1415286784
I0111 22:36:32.142329 32596 layer_factory.hpp:78] Creating layer bn2
I0111 22:36:32.142347 32596 net.cpp:84] Creating Layer bn2
I0111 22:36:32.142356 32596 net.cpp:406] bn2 <- pool1
I0111 22:36:32.142370 32596 net.cpp:367] bn2 -> pool1 (in-place)
I0111 22:36:32.143833 32596 net.cpp:122] Setting up bn2
I0111 22:36:32.143849 32596 net.cpp:129] Top shape: 256 96 27 27 (17915904)
I0111 22:36:32.143856 32596 net.cpp:137] Memory required for data: 1486950400
I0111 22:36:32.143888 32596 layer_factory.hpp:78] Creating layer scale2
I0111 22:36:32.143911 32596 net.cpp:84] Creating Layer scale2
I0111 22:36:32.143919 32596 net.cpp:406] scale2 <- pool1
I0111 22:36:32.143936 32596 net.cpp:367] scale2 -> pool1 (in-place)
I0111 22:36:32.144001 32596 layer_factory.hpp:78] Creating layer scale2
I0111 22:36:32.144182 32596 net.cpp:122] Setting up scale2
I0111 22:36:32.144196 32596 net.cpp:129] Top shape: 256 96 27 27 (17915904)
I0111 22:36:32.144199 32596 net.cpp:137] Memory required for data: 1558614016
I0111 22:36:32.144213 32596 layer_factory.hpp:78] Creating layer binactive1
I0111 22:36:32.144234 32596 net.cpp:84] Creating Layer binactive1
I0111 22:36:32.144243 32596 net.cpp:406] binactive1 <- pool1
I0111 22:36:32.144260 32596 net.cpp:380] binactive1 -> binactive1
I0111 22:36:32.144326 32596 net.cpp:122] Setting up binactive1
I0111 22:36:32.144342 32596 net.cpp:129] Top shape: 256 96 27 27 (17915904)
I0111 22:36:32.144348 32596 net.cpp:137] Memory required for data: 1630277632
I0111 22:36:32.144354 32596 layer_factory.hpp:78] Creating layer conv2
I0111 22:36:32.144392 32596 net.cpp:84] Creating Layer conv2
I0111 22:36:32.144402 32596 net.cpp:406] conv2 <- binactive1
I0111 22:36:32.144421 32596 net.cpp:380] conv2 -> conv2
I0111 22:36:32.201612 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 19668
I0111 22:36:32.201648 32596 net.cpp:122] Setting up conv2
I0111 22:36:32.201660 32596 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I0111 22:36:32.201664 32596 net.cpp:137] Memory required for data: 1821380608
I0111 22:36:32.201680 32596 layer_factory.hpp:78] Creating layer pool2
I0111 22:36:32.201702 32596 net.cpp:84] Creating Layer pool2
I0111 22:36:32.201711 32596 net.cpp:406] pool2 <- conv2
I0111 22:36:32.201731 32596 net.cpp:380] pool2 -> pool2
I0111 22:36:32.201799 32596 net.cpp:122] Setting up pool2
I0111 22:36:32.201812 32596 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0111 22:36:32.201817 32596 net.cpp:137] Memory required for data: 1865682944
I0111 22:36:32.201823 32596 layer_factory.hpp:78] Creating layer bn3
I0111 22:36:32.201839 32596 net.cpp:84] Creating Layer bn3
I0111 22:36:32.201848 32596 net.cpp:406] bn3 <- pool2
I0111 22:36:32.201861 32596 net.cpp:367] bn3 -> pool2 (in-place)
I0111 22:36:32.202085 32596 net.cpp:122] Setting up bn3
I0111 22:36:32.202097 32596 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0111 22:36:32.202100 32596 net.cpp:137] Memory required for data: 1909985280
I0111 22:36:32.202121 32596 layer_factory.hpp:78] Creating layer scale3
I0111 22:36:32.202141 32596 net.cpp:84] Creating Layer scale3
I0111 22:36:32.202148 32596 net.cpp:406] scale3 <- pool2
I0111 22:36:32.202170 32596 net.cpp:367] scale3 -> pool2 (in-place)
I0111 22:36:32.202235 32596 layer_factory.hpp:78] Creating layer scale3
I0111 22:36:32.202399 32596 net.cpp:122] Setting up scale3
I0111 22:36:32.202414 32596 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0111 22:36:32.202417 32596 net.cpp:137] Memory required for data: 1954287616
I0111 22:36:32.202445 32596 layer_factory.hpp:78] Creating layer binactive2
I0111 22:36:32.202461 32596 net.cpp:84] Creating Layer binactive2
I0111 22:36:32.202469 32596 net.cpp:406] binactive2 <- pool2
I0111 22:36:32.202484 32596 net.cpp:380] binactive2 -> binactive2
I0111 22:36:32.202522 32596 net.cpp:122] Setting up binactive2
I0111 22:36:32.202533 32596 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0111 22:36:32.202535 32596 net.cpp:137] Memory required for data: 1998589952
I0111 22:36:32.202540 32596 layer_factory.hpp:78] Creating layer conv3
I0111 22:36:32.202569 32596 net.cpp:84] Creating Layer conv3
I0111 22:36:32.202579 32596 net.cpp:406] conv3 <- binactive2
I0111 22:36:32.202597 32596 net.cpp:380] conv3 -> conv3
I0111 22:36:32.283957 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 18874368
I0111 22:36:32.284317 32596 net.cpp:122] Setting up conv3
I0111 22:36:32.284353 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.284394 32596 net.cpp:137] Memory required for data: 2065043456
I0111 22:36:32.284435 32596 layer_factory.hpp:78] Creating layer bn4
I0111 22:36:32.284526 32596 net.cpp:84] Creating Layer bn4
I0111 22:36:32.284566 32596 net.cpp:406] bn4 <- conv3
I0111 22:36:32.284620 32596 net.cpp:367] bn4 -> conv3 (in-place)
I0111 22:36:32.284883 32596 net.cpp:122] Setting up bn4
I0111 22:36:32.284899 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.284905 32596 net.cpp:137] Memory required for data: 2131496960
I0111 22:36:32.284936 32596 layer_factory.hpp:78] Creating layer scale4
I0111 22:36:32.284991 32596 net.cpp:84] Creating Layer scale4
I0111 22:36:32.285006 32596 net.cpp:406] scale4 <- conv3
I0111 22:36:32.285048 32596 net.cpp:367] scale4 -> conv3 (in-place)
I0111 22:36:32.285161 32596 layer_factory.hpp:78] Creating layer scale4
I0111 22:36:32.285426 32596 net.cpp:122] Setting up scale4
I0111 22:36:32.285442 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.285449 32596 net.cpp:137] Memory required for data: 2197950464
I0111 22:36:32.285470 32596 layer_factory.hpp:78] Creating layer binactive3
I0111 22:36:32.285511 32596 net.cpp:84] Creating Layer binactive3
I0111 22:36:32.285537 32596 net.cpp:406] binactive3 <- conv3
I0111 22:36:32.285583 32596 net.cpp:380] binactive3 -> binactive3
I0111 22:36:32.285643 32596 net.cpp:122] Setting up binactive3
I0111 22:36:32.285660 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.285681 32596 net.cpp:137] Memory required for data: 2264403968
I0111 22:36:32.285703 32596 layer_factory.hpp:78] Creating layer conv4
I0111 22:36:32.285759 32596 net.cpp:84] Creating Layer conv4
I0111 22:36:32.285773 32596 net.cpp:406] conv4 <- binactive3
I0111 22:36:32.285806 32596 net.cpp:380] conv4 -> conv4
I0111 22:36:32.414640 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 9840
I0111 22:36:32.414707 32596 net.cpp:122] Setting up conv4
I0111 22:36:32.414734 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.414836 32596 net.cpp:137] Memory required for data: 2330857472
I0111 22:36:32.414898 32596 layer_factory.hpp:78] Creating layer bn5
I0111 22:36:32.414975 32596 net.cpp:84] Creating Layer bn5
I0111 22:36:32.415004 32596 net.cpp:406] bn5 <- conv4
I0111 22:36:32.415076 32596 net.cpp:367] bn5 -> conv4 (in-place)
I0111 22:36:32.415386 32596 net.cpp:122] Setting up bn5
I0111 22:36:32.415405 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.415412 32596 net.cpp:137] Memory required for data: 2397310976
I0111 22:36:32.415448 32596 layer_factory.hpp:78] Creating layer scale5
I0111 22:36:32.415506 32596 net.cpp:84] Creating Layer scale5
I0111 22:36:32.415524 32596 net.cpp:406] scale5 <- conv4
I0111 22:36:32.415558 32596 net.cpp:367] scale5 -> conv4 (in-place)
I0111 22:36:32.415678 32596 layer_factory.hpp:78] Creating layer scale5
I0111 22:36:32.415886 32596 net.cpp:122] Setting up scale5
I0111 22:36:32.415902 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.415908 32596 net.cpp:137] Memory required for data: 2463764480
I0111 22:36:32.415932 32596 layer_factory.hpp:78] Creating layer binactive4
I0111 22:36:32.415977 32596 net.cpp:84] Creating Layer binactive4
I0111 22:36:32.416007 32596 net.cpp:406] binactive4 <- conv4
I0111 22:36:32.416054 32596 net.cpp:380] binactive4 -> binactive4
I0111 22:36:32.416132 32596 net.cpp:122] Setting up binactive4
I0111 22:36:32.416157 32596 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0111 22:36:32.416183 32596 net.cpp:137] Memory required for data: 2530217984
I0111 22:36:32.416211 32596 layer_factory.hpp:78] Creating layer conv5
I0111 22:36:32.416278 32596 net.cpp:84] Creating Layer conv5
I0111 22:36:32.416293 32596 net.cpp:406] conv5 <- binactive4
I0111 22:36:32.416323 32596 net.cpp:380] conv5 -> conv5
I0111 22:36:32.500063 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 18874368
I0111 22:36:32.500488 32596 net.cpp:122] Setting up conv5
I0111 22:36:32.500535 32596 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0111 22:36:32.500641 32596 net.cpp:137] Memory required for data: 2574520320
I0111 22:36:32.500718 32596 layer_factory.hpp:78] Creating layer pool5
I0111 22:36:32.500784 32596 net.cpp:84] Creating Layer pool5
I0111 22:36:32.500825 32596 net.cpp:406] pool5 <- conv5
I0111 22:36:32.500888 32596 net.cpp:380] pool5 -> pool5
I0111 22:36:32.501022 32596 net.cpp:122] Setting up pool5
I0111 22:36:32.501044 32596 net.cpp:129] Top shape: 256 256 6 6 (2359296)
I0111 22:36:32.501070 32596 net.cpp:137] Memory required for data: 2583957504
I0111 22:36:32.501101 32596 layer_factory.hpp:78] Creating layer bn6
I0111 22:36:32.501148 32596 net.cpp:84] Creating Layer bn6
I0111 22:36:32.501164 32596 net.cpp:406] bn6 <- pool5
I0111 22:36:32.501216 32596 net.cpp:367] bn6 -> pool5 (in-place)
I0111 22:36:32.501518 32596 net.cpp:122] Setting up bn6
I0111 22:36:32.501539 32596 net.cpp:129] Top shape: 256 256 6 6 (2359296)
I0111 22:36:32.501590 32596 net.cpp:137] Memory required for data: 2593394688
I0111 22:36:32.501677 32596 layer_factory.hpp:78] Creating layer scale6
I0111 22:36:32.501720 32596 net.cpp:84] Creating Layer scale6
I0111 22:36:32.501758 32596 net.cpp:406] scale6 <- pool5
I0111 22:36:32.501814 32596 net.cpp:367] scale6 -> pool5 (in-place)
I0111 22:36:32.501931 32596 layer_factory.hpp:78] Creating layer scale6
I0111 22:36:32.502140 32596 net.cpp:122] Setting up scale6
I0111 22:36:32.502159 32596 net.cpp:129] Top shape: 256 256 6 6 (2359296)
I0111 22:36:32.502166 32596 net.cpp:137] Memory required for data: 2602831872
I0111 22:36:32.502192 32596 layer_factory.hpp:78] Creating layer binactive5
I0111 22:36:32.502249 32596 net.cpp:84] Creating Layer binactive5
I0111 22:36:32.502282 32596 net.cpp:406] binactive5 <- pool5
I0111 22:36:32.502336 32596 net.cpp:380] binactive5 -> binactive5
I0111 22:36:32.502414 32596 net.cpp:122] Setting up binactive5
I0111 22:36:32.502434 32596 net.cpp:129] Top shape: 256 256 6 6 (2359296)
I0111 22:36:32.502460 32596 net.cpp:137] Memory required for data: 2612269056
I0111 22:36:32.502486 32596 layer_factory.hpp:78] Creating layer fc6
I0111 22:36:32.502550 32596 net.cpp:84] Creating Layer fc6
I0111 22:36:32.502564 32596 net.cpp:406] fc6 <- binactive5
I0111 22:36:32.502620 32596 net.cpp:380] fc6 -> fc6
I0111 22:36:35.922567 32596 net.cpp:122] Setting up fc6
I0111 22:36:35.922621 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:35.922628 32596 net.cpp:137] Memory required for data: 2616463360
I0111 22:36:35.922706 32596 layer_factory.hpp:78] Creating layer bn7
I0111 22:36:35.922760 32596 net.cpp:84] Creating Layer bn7
I0111 22:36:35.922786 32596 net.cpp:406] bn7 <- fc6
I0111 22:36:35.922823 32596 net.cpp:367] bn7 -> fc6 (in-place)
I0111 22:36:35.923097 32596 net.cpp:122] Setting up bn7
I0111 22:36:35.923110 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:35.923116 32596 net.cpp:137] Memory required for data: 2620657664
I0111 22:36:35.923178 32596 layer_factory.hpp:78] Creating layer scale7
I0111 22:36:35.923207 32596 net.cpp:84] Creating Layer scale7
I0111 22:36:35.923218 32596 net.cpp:406] scale7 <- fc6
I0111 22:36:35.923243 32596 net.cpp:367] scale7 -> fc6 (in-place)
I0111 22:36:35.923332 32596 layer_factory.hpp:78] Creating layer scale7
I0111 22:36:35.923534 32596 net.cpp:122] Setting up scale7
I0111 22:36:35.923548 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:35.923553 32596 net.cpp:137] Memory required for data: 2624851968
I0111 22:36:35.923569 32596 layer_factory.hpp:78] Creating layer binactive6
I0111 22:36:35.923583 32596 net.cpp:84] Creating Layer binactive6
I0111 22:36:35.923590 32596 net.cpp:406] binactive6 <- fc6
I0111 22:36:35.923607 32596 net.cpp:380] binactive6 -> binactive6
I0111 22:36:35.923661 32596 net.cpp:122] Setting up binactive6
I0111 22:36:35.923673 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:35.923678 32596 net.cpp:137] Memory required for data: 2629046272
I0111 22:36:35.923683 32596 layer_factory.hpp:78] Creating layer fc7
I0111 22:36:35.923719 32596 net.cpp:84] Creating Layer fc7
I0111 22:36:35.923728 32596 net.cpp:406] fc7 <- binactive6
I0111 22:36:35.923748 32596 net.cpp:380] fc7 -> fc7
I0111 22:36:37.307634 32596 net.cpp:122] Setting up fc7
I0111 22:36:37.307685 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:37.307689 32596 net.cpp:137] Memory required for data: 2633240576
I0111 22:36:37.307723 32596 layer_factory.hpp:78] Creating layer bn8
I0111 22:36:37.307793 32596 net.cpp:84] Creating Layer bn8
I0111 22:36:37.307807 32596 net.cpp:406] bn8 <- fc7
I0111 22:36:37.307834 32596 net.cpp:367] bn8 -> fc7 (in-place)
I0111 22:36:37.308102 32596 net.cpp:122] Setting up bn8
I0111 22:36:37.308128 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:37.308131 32596 net.cpp:137] Memory required for data: 2637434880
I0111 22:36:37.308168 32596 layer_factory.hpp:78] Creating layer scale8
I0111 22:36:37.308190 32596 net.cpp:84] Creating Layer scale8
I0111 22:36:37.308197 32596 net.cpp:406] scale8 <- fc7
I0111 22:36:37.308235 32596 net.cpp:367] scale8 -> fc7 (in-place)
I0111 22:36:37.308313 32596 layer_factory.hpp:78] Creating layer scale8
I0111 22:36:37.308512 32596 net.cpp:122] Setting up scale8
I0111 22:36:37.308527 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:37.308532 32596 net.cpp:137] Memory required for data: 2641629184
I0111 22:36:37.308563 32596 layer_factory.hpp:78] Creating layer relu7
I0111 22:36:37.308579 32596 net.cpp:84] Creating Layer relu7
I0111 22:36:37.308586 32596 net.cpp:406] relu7 <- fc7
I0111 22:36:37.308601 32596 net.cpp:367] relu7 -> fc7 (in-place)
I0111 22:36:37.309533 32596 net.cpp:122] Setting up relu7
I0111 22:36:37.309547 32596 net.cpp:129] Top shape: 256 4096 (1048576)
I0111 22:36:37.309567 32596 net.cpp:137] Memory required for data: 2645823488
I0111 22:36:37.309573 32596 layer_factory.hpp:78] Creating layer fc8
I0111 22:36:37.309626 32596 net.cpp:84] Creating Layer fc8
I0111 22:36:37.309635 32596 net.cpp:406] fc8 <- fc7
I0111 22:36:37.309656 32596 net.cpp:380] fc8 -> fc8
I0111 22:36:37.638682 32596 net.cpp:122] Setting up fc8
I0111 22:36:37.638746 32596 net.cpp:129] Top shape: 256 1000 (256000)
I0111 22:36:37.638753 32596 net.cpp:137] Memory required for data: 2646847488
I0111 22:36:37.638789 32596 layer_factory.hpp:78] Creating layer fc8_fc8_0_split
I0111 22:36:37.638842 32596 net.cpp:84] Creating Layer fc8_fc8_0_split
I0111 22:36:37.638872 32596 net.cpp:406] fc8_fc8_0_split <- fc8
I0111 22:36:37.638898 32596 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0111 22:36:37.638926 32596 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0111 22:36:37.638940 32596 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_2
I0111 22:36:37.639014 32596 net.cpp:122] Setting up fc8_fc8_0_split
I0111 22:36:37.639027 32596 net.cpp:129] Top shape: 256 1000 (256000)
I0111 22:36:37.639032 32596 net.cpp:129] Top shape: 256 1000 (256000)
I0111 22:36:37.639036 32596 net.cpp:129] Top shape: 256 1000 (256000)
I0111 22:36:37.639040 32596 net.cpp:137] Memory required for data: 2649919488
I0111 22:36:37.639046 32596 layer_factory.hpp:78] Creating layer loss
I0111 22:36:37.639071 32596 net.cpp:84] Creating Layer loss
I0111 22:36:37.639080 32596 net.cpp:406] loss <- fc8_fc8_0_split_0
I0111 22:36:37.639091 32596 net.cpp:406] loss <- label_data_1_split_0
I0111 22:36:37.639104 32596 net.cpp:380] loss -> loss
I0111 22:36:37.639142 32596 layer_factory.hpp:78] Creating layer loss
I0111 22:36:37.642961 32596 net.cpp:122] Setting up loss
I0111 22:36:37.642997 32596 net.cpp:129] Top shape: (1)
I0111 22:36:37.643000 32596 net.cpp:132]     with loss weight 1
I0111 22:36:37.643041 32596 net.cpp:137] Memory required for data: 2649919492
I0111 22:36:37.643064 32596 layer_factory.hpp:78] Creating layer accuracy
I0111 22:36:37.643101 32596 net.cpp:84] Creating Layer accuracy
I0111 22:36:37.643113 32596 net.cpp:406] accuracy <- fc8_fc8_0_split_1
I0111 22:36:37.643131 32596 net.cpp:406] accuracy <- label_data_1_split_1
I0111 22:36:37.643146 32596 net.cpp:380] accuracy -> accuracy
I0111 22:36:37.643172 32596 net.cpp:122] Setting up accuracy
I0111 22:36:37.643182 32596 net.cpp:129] Top shape: (1)
I0111 22:36:37.643185 32596 net.cpp:137] Memory required for data: 2649919496
I0111 22:36:37.643190 32596 layer_factory.hpp:78] Creating layer accuracy_5
I0111 22:36:37.643205 32596 net.cpp:84] Creating Layer accuracy_5
I0111 22:36:37.643211 32596 net.cpp:406] accuracy_5 <- fc8_fc8_0_split_2
I0111 22:36:37.643223 32596 net.cpp:406] accuracy_5 <- label_data_1_split_2
I0111 22:36:37.643234 32596 net.cpp:380] accuracy_5 -> accuracy_5
I0111 22:36:37.643260 32596 net.cpp:122] Setting up accuracy_5
I0111 22:36:37.643270 32596 net.cpp:129] Top shape: (1)
I0111 22:36:37.643275 32596 net.cpp:137] Memory required for data: 2649919500
I0111 22:36:37.643281 32596 net.cpp:200] accuracy_5 does not need backward computation.
I0111 22:36:37.643287 32596 net.cpp:200] accuracy does not need backward computation.
I0111 22:36:37.643292 32596 net.cpp:198] loss needs backward computation.
I0111 22:36:37.643297 32596 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0111 22:36:37.643323 32596 net.cpp:198] fc8 needs backward computation.
I0111 22:36:37.643328 32596 net.cpp:198] relu7 needs backward computation.
I0111 22:36:37.643333 32596 net.cpp:198] scale8 needs backward computation.
I0111 22:36:37.643337 32596 net.cpp:198] bn8 needs backward computation.
I0111 22:36:37.643347 32596 net.cpp:198] fc7 needs backward computation.
I0111 22:36:37.643352 32596 net.cpp:198] binactive6 needs backward computation.
I0111 22:36:37.643355 32596 net.cpp:198] scale7 needs backward computation.
I0111 22:36:37.643360 32596 net.cpp:198] bn7 needs backward computation.
I0111 22:36:37.643363 32596 net.cpp:198] fc6 needs backward computation.
I0111 22:36:37.643368 32596 net.cpp:198] binactive5 needs backward computation.
I0111 22:36:37.643373 32596 net.cpp:198] scale6 needs backward computation.
I0111 22:36:37.643376 32596 net.cpp:198] bn6 needs backward computation.
I0111 22:36:37.643380 32596 net.cpp:198] pool5 needs backward computation.
I0111 22:36:37.643384 32596 net.cpp:198] conv5 needs backward computation.
I0111 22:36:37.643389 32596 net.cpp:198] binactive4 needs backward computation.
I0111 22:36:37.643394 32596 net.cpp:198] scale5 needs backward computation.
I0111 22:36:37.643399 32596 net.cpp:198] bn5 needs backward computation.
I0111 22:36:37.643402 32596 net.cpp:198] conv4 needs backward computation.
I0111 22:36:37.643409 32596 net.cpp:198] binactive3 needs backward computation.
I0111 22:36:37.643412 32596 net.cpp:198] scale4 needs backward computation.
I0111 22:36:37.643416 32596 net.cpp:198] bn4 needs backward computation.
I0111 22:36:37.643421 32596 net.cpp:198] conv3 needs backward computation.
I0111 22:36:37.643425 32596 net.cpp:198] binactive2 needs backward computation.
I0111 22:36:37.643431 32596 net.cpp:198] scale3 needs backward computation.
I0111 22:36:37.643435 32596 net.cpp:198] bn3 needs backward computation.
I0111 22:36:37.643440 32596 net.cpp:198] pool2 needs backward computation.
I0111 22:36:37.643446 32596 net.cpp:198] conv2 needs backward computation.
I0111 22:36:37.643451 32596 net.cpp:198] binactive1 needs backward computation.
I0111 22:36:37.643456 32596 net.cpp:198] scale2 needs backward computation.
I0111 22:36:37.643460 32596 net.cpp:198] bn2 needs backward computation.
I0111 22:36:37.643465 32596 net.cpp:198] pool1 needs backward computation.
I0111 22:36:37.643470 32596 net.cpp:198] relu1 needs backward computation.
I0111 22:36:37.643476 32596 net.cpp:198] scale1 needs backward computation.
I0111 22:36:37.643479 32596 net.cpp:198] bn1 needs backward computation.
I0111 22:36:37.643483 32596 net.cpp:198] conv1 needs backward computation.
I0111 22:36:37.643491 32596 net.cpp:200] label_data_1_split does not need backward computation.
I0111 22:36:37.643496 32596 net.cpp:200] data does not need backward computation.
I0111 22:36:37.643507 32596 net.cpp:242] This network produces output accuracy
I0111 22:36:37.643532 32596 net.cpp:242] This network produces output accuracy_5
I0111 22:36:37.643553 32596 net.cpp:242] This network produces output loss
I0111 22:36:37.643610 32596 net.cpp:255] Network initialization done.
I0111 22:36:37.644488 32596 solver.cpp:193] Creating test net (#0) specified by net file: train_test.prototxt
I0111 22:36:37.644611 32596 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0111 22:36:37.644817 32596 net.cpp:51] Initializing net from parameters: 
name: "AlexNet-BN"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 104
    mean_value: 117
    mean_value: 123
  }
  data_param {
    source: "/home/zhengzhe/Data/imagenet_shrt256/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "pool1"
  top: "pool1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive1"
  type: "BinActive"
  bottom: "pool1"
  top: "binactive1"
}
layer {
  name: "conv2"
  type: "BinaryConvolution"
  bottom: "binactive1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "pool2"
  top: "pool2"
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "pool2"
  top: "pool2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive2"
  type: "BinActive"
  bottom: "pool2"
  top: "binactive2"
}
layer {
  name: "conv3"
  type: "BinaryConvolution"
  bottom: "binactive2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive3"
  type: "BinActive"
  bottom: "conv3"
  top: "binactive3"
}
layer {
  name: "conv4"
  type: "BinaryConvolution"
  bottom: "binactive3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive4"
  type: "BinActive"
  bottom: "conv4"
  top: "binactive4"
}
layer {
  name: "conv5"
  type: "BinaryConvolution"
  bottom: "binactive4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "bn6"
  type: "BatchNorm"
  bottom: "pool5"
  top: "pool5"
}
layer {
  name: "scale6"
  type: "Scale"
  bottom: "pool5"
  top: "pool5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive5"
  type: "BinActive"
  bottom: "pool5"
  top: "binactive5"
}
layer {
  name: "fc6"
  type: "BinaryInnerProduct"
  bottom: "binactive5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4096
    bias_term: false
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn7"
  type: "BatchNorm"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "scale7"
  type: "Scale"
  bottom: "fc6"
  top: "fc6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "binactive6"
  type: "BinActive"
  bottom: "fc6"
  top: "binactive6"
}
layer {
  name: "fc7"
  type: "BinaryInnerProduct"
  bottom: "binactive6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  inner_product_param {
    num_output: 4096
    bias_term: false
    weight_filler {
      type: "msra"
    }
  }
  debug_param {
    xnorno_grad: true
  }
}
layer {
  name: "bn8"
  type: "BatchNorm"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "scale8"
  type: "Scale"
  bottom: "fc7"
  top: "fc7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "accuracy_5"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_5"
  accuracy_param {
    top_k: 5
  }
}
I0111 22:36:37.645211 32596 layer_factory.hpp:78] Creating layer data
I0111 22:36:37.645323 32596 db_lmdb.cpp:35] Opened lmdb /home/zhengzhe/Data/imagenet_shrt256/ilsvrc12_val_lmdb
I0111 22:36:37.645366 32596 net.cpp:84] Creating Layer data
I0111 22:36:37.645383 32596 net.cpp:380] data -> data
I0111 22:36:37.645416 32596 net.cpp:380] data -> label
I0111 22:36:37.645984 32596 data_layer.cpp:45] output data size: 50,3,224,224
I0111 22:36:37.724177 32596 base_data_layer.cpp:72] Initializing prefetch
I0111 22:36:37.887684 32596 base_data_layer.cpp:75] Prefetch initialized.
I0111 22:36:37.887725 32596 net.cpp:122] Setting up data
I0111 22:36:37.887751 32596 net.cpp:129] Top shape: 50 3 224 224 (7526400)
I0111 22:36:37.887759 32596 net.cpp:129] Top shape: 50 (50)
I0111 22:36:37.887763 32596 net.cpp:137] Memory required for data: 30105800
I0111 22:36:37.887791 32596 layer_factory.hpp:78] Creating layer label_data_1_split
I0111 22:36:37.887856 32596 net.cpp:84] Creating Layer label_data_1_split
I0111 22:36:37.887887 32596 net.cpp:406] label_data_1_split <- label
I0111 22:36:37.887919 32596 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0111 22:36:37.887959 32596 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0111 22:36:37.887974 32596 net.cpp:380] label_data_1_split -> label_data_1_split_2
I0111 22:36:37.888224 32596 net.cpp:122] Setting up label_data_1_split
I0111 22:36:37.888238 32596 net.cpp:129] Top shape: 50 (50)
I0111 22:36:37.888244 32596 net.cpp:129] Top shape: 50 (50)
I0111 22:36:37.888249 32596 net.cpp:129] Top shape: 50 (50)
I0111 22:36:37.888252 32596 net.cpp:137] Memory required for data: 30106400
I0111 22:36:37.888257 32596 layer_factory.hpp:78] Creating layer conv1
I0111 22:36:37.888298 32596 net.cpp:84] Creating Layer conv1
I0111 22:36:37.888308 32596 net.cpp:406] conv1 <- data
I0111 22:36:37.888325 32596 net.cpp:380] conv1 -> conv1
I0111 22:36:37.896502 32596 cudnn_conv_layer.cpp:194] Reallocating workspace storage: 127416
I0111 22:36:37.896562 32596 net.cpp:122] Setting up conv1
I0111 22:36:37.896576 32596 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0111 22:36:37.896580 32596 net.cpp:137] Memory required for data: 88186400
I0111 22:36:37.896627 32596 layer_factory.hpp:78] Creating layer bn1
I0111 22:36:37.896648 32596 net.cpp:84] Creating Layer bn1
I0111 22:36:37.896657 32596 net.cpp:406] bn1 <- conv1
I0111 22:36:37.896672 32596 net.cpp:367] bn1 -> conv1 (in-place)
I0111 22:36:37.896947 32596 net.cpp:122] Setting up bn1
I0111 22:36:37.896960 32596 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0111 22:36:37.896962 32596 net.cpp:137] Memory required for data: 146266400
I0111 22:36:37.896996 32596 layer_factory.hpp:78] Creating layer scale1
I0111 22:36:37.897022 32596 net.cpp:84] Creating Layer scale1
I0111 22:36:37.897029 32596 net.cpp:406] scale1 <- conv1
I0111 22:36:37.897043 32596 net.cpp:367] scale1 -> conv1 (in-place)
I0111 22:36:37.897119 32596 layer_factory.hpp:78] Creating layer scale1
I0111 22:36:37.897363 32596 net.cpp:122] Setting up scale1
I0111 22:36:37.897377 32596 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0111 22:36:37.897380 32596 net.cpp:137] Memory required for data: 204346400
I0111 22:36:37.897405 32596 layer_factory.hpp:78] Creating layer relu1
I0111 22:36:37.897423 32596 net.cpp:84] Creating Layer relu1
I0111 22:36:37.897431 32596 net.cpp:406] relu1 <- conv1
I0111 22:36:37.897449 32596 net.cpp:367] relu1 -> conv1 (in-place)
I0111 22:36:37.898059 32596 net.cpp:122] Setting up relu1
I0111 22:36:37.898072 32596 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0111 22:36:37.898077 32596 net.cpp:137] Memory required for data: 262426400
I0111 22:36:37.898082 32596 layer_factory.hpp:78] Creating layer pool1
I0111 22:36:37.898102 32596 net.cpp:84] Creating Layer pool1
I0111 22:36:37.898124 32596 net.cpp:406] pool1 <- conv1
I0111 22:36:37.898144 32596 net.cpp:380] pool1 -> pool1
I0111 22:36:37.898217 32596 net.cpp:122] Setting up pool1
I0111 22:36:37.898233 32596 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I0111 22:36:37.898238 32596 net.cpp:137] Memory required for data: 276423200
I0111 22:36:37.898244 32596 layer_factory.hpp:78] Creating layer bn2
I0111 22:36:37.898258 32596 net.cpp:84] Creating Layer bn2
I0111 22:36:37.898264 32596 net.cpp:406] bn2 <- pool1
I0111 22:36:37.898277 32596 net.cpp:367] bn2 -> pool1 (in-place)
I0111 22:36:37.898546 32596 net.cpp:122] Setting up bn2
I0111 22:36:37.898558 32596 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I0111 22:36:37.898561 32596 net.cpp:137] Memory required for data: 290420000
I0111 22:36:37.898592 32596 layer_factory.hpp:78] Creating layer scale2
I0111 22:36:37.898614 32596 net.cpp:84] Creating Layer scale2
I0111 22:36:37.898623 32596 net.cpp:406] scale2 <- pool1
I0111 22:36:37.898640 32596 net.cpp:367] scale2 -> pool1 (in-place)
I0111 22:36:37.898711 32596 layer_factory.hpp:78] Creating layer scale2
I0111 22:36:37.898912 32596 net.cpp:122] Setting up scale2
I0111 22:36:37.898926 32596 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I0111 22:36:37.898928 32596 net.cpp:137] Memory required for data: 304416800
I0111 22:36:37.898943 32596 layer_factory.hpp:78] Creating layer binactive1
I0111 22:36:37.898962 32596 net.cpp:84] Creating Layer binactive1
I0111 22:36:37.898969 32596 net.cpp:406] binactive1 <- pool1
I0111 22:36:37.898983 32596 net.cpp:380] binactive1 -> binactive1
I0111 22:36:37.899026 32596 net.cpp:122] Setting up binactive1
I0111 22:36:37.899039 32596 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I0111 22:36:37.899042 32596 net.cpp:137] Memory required for data: 318413600
I0111 22:36:37.899047 32596 layer_factory.hpp:78] Creating layer conv2
I0111 22:36:37.899075 32596 net.cpp:84] Creating Layer conv2
I0111 22:36:37.899082 32596 net.cpp:406] conv2 <- binactive1
I0111 22:36:37.899101 32596 net.cpp:380] conv2 -> conv2
I0111 22:36:37.956254 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 19668
I0111 22:36:37.956297 32596 net.cpp:122] Setting up conv2
I0111 22:36:37.956308 32596 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I0111 22:36:37.956312 32596 net.cpp:137] Memory required for data: 355738400
I0111 22:36:37.956326 32596 layer_factory.hpp:78] Creating layer pool2
I0111 22:36:37.956352 32596 net.cpp:84] Creating Layer pool2
I0111 22:36:37.956378 32596 net.cpp:406] pool2 <- conv2
I0111 22:36:37.956399 32596 net.cpp:380] pool2 -> pool2
I0111 22:36:37.956468 32596 net.cpp:122] Setting up pool2
I0111 22:36:37.956485 32596 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0111 22:36:37.956487 32596 net.cpp:137] Memory required for data: 364391200
I0111 22:36:37.956493 32596 layer_factory.hpp:78] Creating layer bn3
I0111 22:36:37.956506 32596 net.cpp:84] Creating Layer bn3
I0111 22:36:37.956513 32596 net.cpp:406] bn3 <- pool2
I0111 22:36:37.956527 32596 net.cpp:367] bn3 -> pool2 (in-place)
I0111 22:36:37.956773 32596 net.cpp:122] Setting up bn3
I0111 22:36:37.956786 32596 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0111 22:36:37.956790 32596 net.cpp:137] Memory required for data: 373044000
I0111 22:36:37.956835 32596 layer_factory.hpp:78] Creating layer scale3
I0111 22:36:37.956853 32596 net.cpp:84] Creating Layer scale3
I0111 22:36:37.956861 32596 net.cpp:406] scale3 <- pool2
I0111 22:36:37.956876 32596 net.cpp:367] scale3 -> pool2 (in-place)
I0111 22:36:37.956950 32596 layer_factory.hpp:78] Creating layer scale3
I0111 22:36:37.957125 32596 net.cpp:122] Setting up scale3
I0111 22:36:37.957139 32596 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0111 22:36:37.957141 32596 net.cpp:137] Memory required for data: 381696800
I0111 22:36:37.957166 32596 layer_factory.hpp:78] Creating layer binactive2
I0111 22:36:37.957182 32596 net.cpp:84] Creating Layer binactive2
I0111 22:36:37.957190 32596 net.cpp:406] binactive2 <- pool2
I0111 22:36:37.957204 32596 net.cpp:380] binactive2 -> binactive2
I0111 22:36:37.957254 32596 net.cpp:122] Setting up binactive2
I0111 22:36:37.957267 32596 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0111 22:36:37.957270 32596 net.cpp:137] Memory required for data: 390349600
I0111 22:36:37.957275 32596 layer_factory.hpp:78] Creating layer conv3
I0111 22:36:37.957299 32596 net.cpp:84] Creating Layer conv3
I0111 22:36:37.957309 32596 net.cpp:406] conv3 <- binactive2
I0111 22:36:37.957329 32596 net.cpp:380] conv3 -> conv3
I0111 22:36:38.035863 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 18874368
I0111 22:36:38.036221 32596 net.cpp:122] Setting up conv3
I0111 22:36:38.036240 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.036245 32596 net.cpp:137] Memory required for data: 403328800
I0111 22:36:38.036267 32596 layer_factory.hpp:78] Creating layer bn4
I0111 22:36:38.036296 32596 net.cpp:84] Creating Layer bn4
I0111 22:36:38.036307 32596 net.cpp:406] bn4 <- conv3
I0111 22:36:38.036329 32596 net.cpp:367] bn4 -> conv3 (in-place)
I0111 22:36:38.036592 32596 net.cpp:122] Setting up bn4
I0111 22:36:38.036604 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.036607 32596 net.cpp:137] Memory required for data: 416308000
I0111 22:36:38.036628 32596 layer_factory.hpp:78] Creating layer scale4
I0111 22:36:38.036649 32596 net.cpp:84] Creating Layer scale4
I0111 22:36:38.036656 32596 net.cpp:406] scale4 <- conv3
I0111 22:36:38.036674 32596 net.cpp:367] scale4 -> conv3 (in-place)
I0111 22:36:38.036742 32596 layer_factory.hpp:78] Creating layer scale4
I0111 22:36:38.036921 32596 net.cpp:122] Setting up scale4
I0111 22:36:38.036934 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.036937 32596 net.cpp:137] Memory required for data: 429287200
I0111 22:36:38.036952 32596 layer_factory.hpp:78] Creating layer binactive3
I0111 22:36:38.036967 32596 net.cpp:84] Creating Layer binactive3
I0111 22:36:38.036974 32596 net.cpp:406] binactive3 <- conv3
I0111 22:36:38.036993 32596 net.cpp:380] binactive3 -> binactive3
I0111 22:36:38.037037 32596 net.cpp:122] Setting up binactive3
I0111 22:36:38.037050 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.037053 32596 net.cpp:137] Memory required for data: 442266400
I0111 22:36:38.037058 32596 layer_factory.hpp:78] Creating layer conv4
I0111 22:36:38.037091 32596 net.cpp:84] Creating Layer conv4
I0111 22:36:38.037099 32596 net.cpp:406] conv4 <- binactive3
I0111 22:36:38.037118 32596 net.cpp:380] conv4 -> conv4
I0111 22:36:38.156479 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 9840
I0111 22:36:38.156540 32596 net.cpp:122] Setting up conv4
I0111 22:36:38.156565 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.156569 32596 net.cpp:137] Memory required for data: 455245600
I0111 22:36:38.156599 32596 layer_factory.hpp:78] Creating layer bn5
I0111 22:36:38.156637 32596 net.cpp:84] Creating Layer bn5
I0111 22:36:38.156651 32596 net.cpp:406] bn5 <- conv4
I0111 22:36:38.156674 32596 net.cpp:367] bn5 -> conv4 (in-place)
I0111 22:36:38.156966 32596 net.cpp:122] Setting up bn5
I0111 22:36:38.156980 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.156985 32596 net.cpp:137] Memory required for data: 468224800
I0111 22:36:38.157030 32596 layer_factory.hpp:78] Creating layer scale5
I0111 22:36:38.157054 32596 net.cpp:84] Creating Layer scale5
I0111 22:36:38.157063 32596 net.cpp:406] scale5 <- conv4
I0111 22:36:38.157078 32596 net.cpp:367] scale5 -> conv4 (in-place)
I0111 22:36:38.157156 32596 layer_factory.hpp:78] Creating layer scale5
I0111 22:36:38.157348 32596 net.cpp:122] Setting up scale5
I0111 22:36:38.157362 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.157366 32596 net.cpp:137] Memory required for data: 481204000
I0111 22:36:38.157380 32596 layer_factory.hpp:78] Creating layer binactive4
I0111 22:36:38.157397 32596 net.cpp:84] Creating Layer binactive4
I0111 22:36:38.157403 32596 net.cpp:406] binactive4 <- conv4
I0111 22:36:38.157418 32596 net.cpp:380] binactive4 -> binactive4
I0111 22:36:38.157464 32596 net.cpp:122] Setting up binactive4
I0111 22:36:38.157481 32596 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0111 22:36:38.157485 32596 net.cpp:137] Memory required for data: 494183200
I0111 22:36:38.157490 32596 layer_factory.hpp:78] Creating layer conv5
I0111 22:36:38.157521 32596 net.cpp:84] Creating Layer conv5
I0111 22:36:38.157529 32596 net.cpp:406] conv5 <- binactive4
I0111 22:36:38.157546 32596 net.cpp:380] conv5 -> conv5
I0111 22:36:38.237684 32596 cudnn_binary_conv_layer.cpp:194] Reallocating workspace storage: 18874368
I0111 22:36:38.238032 32596 net.cpp:122] Setting up conv5
I0111 22:36:38.238054 32596 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0111 22:36:38.238060 32596 net.cpp:137] Memory required for data: 502836000
I0111 22:36:38.238085 32596 layer_factory.hpp:78] Creating layer pool5
I0111 22:36:38.238117 32596 net.cpp:84] Creating Layer pool5
I0111 22:36:38.238129 32596 net.cpp:406] pool5 <- conv5
I0111 22:36:38.238152 32596 net.cpp:380] pool5 -> pool5
I0111 22:36:38.238243 32596 net.cpp:122] Setting up pool5
I0111 22:36:38.238258 32596 net.cpp:129] Top shape: 50 256 6 6 (460800)
I0111 22:36:38.238262 32596 net.cpp:137] Memory required for data: 504679200
I0111 22:36:38.238267 32596 layer_factory.hpp:78] Creating layer bn6
I0111 22:36:38.238284 32596 net.cpp:84] Creating Layer bn6
I0111 22:36:38.238293 32596 net.cpp:406] bn6 <- pool5
I0111 22:36:38.238307 32596 net.cpp:367] bn6 -> pool5 (in-place)
I0111 22:36:38.238649 32596 net.cpp:122] Setting up bn6
I0111 22:36:38.238662 32596 net.cpp:129] Top shape: 50 256 6 6 (460800)
I0111 22:36:38.238665 32596 net.cpp:137] Memory required for data: 506522400
I0111 22:36:38.238703 32596 layer_factory.hpp:78] Creating layer scale6
I0111 22:36:38.238724 32596 net.cpp:84] Creating Layer scale6
I0111 22:36:38.238734 32596 net.cpp:406] scale6 <- pool5
I0111 22:36:38.238749 32596 net.cpp:367] scale6 -> pool5 (in-place)
I0111 22:36:38.238829 32596 layer_factory.hpp:78] Creating layer scale6
I0111 22:36:38.239012 32596 net.cpp:122] Setting up scale6
I0111 22:36:38.239024 32596 net.cpp:129] Top shape: 50 256 6 6 (460800)
I0111 22:36:38.239027 32596 net.cpp:137] Memory required for data: 508365600
I0111 22:36:38.239042 32596 layer_factory.hpp:78] Creating layer binactive5
I0111 22:36:38.239056 32596 net.cpp:84] Creating Layer binactive5
I0111 22:36:38.239063 32596 net.cpp:406] binactive5 <- pool5
I0111 22:36:38.239078 32596 net.cpp:380] binactive5 -> binactive5
I0111 22:36:38.239123 32596 net.cpp:122] Setting up binactive5
I0111 22:36:38.239136 32596 net.cpp:129] Top shape: 50 256 6 6 (460800)
I0111 22:36:38.239140 32596 net.cpp:137] Memory required for data: 510208800
I0111 22:36:38.239145 32596 layer_factory.hpp:78] Creating layer fc6
I0111 22:36:38.239167 32596 net.cpp:84] Creating Layer fc6
I0111 22:36:38.239176 32596 net.cpp:406] fc6 <- binactive5
I0111 22:36:38.239193 32596 net.cpp:380] fc6 -> fc6
I0111 22:36:41.470916 32596 net.cpp:122] Setting up fc6
I0111 22:36:41.470957 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:41.470963 32596 net.cpp:137] Memory required for data: 511028000
I0111 22:36:41.471010 32596 layer_factory.hpp:78] Creating layer bn7
I0111 22:36:41.471082 32596 net.cpp:84] Creating Layer bn7
I0111 22:36:41.471097 32596 net.cpp:406] bn7 <- fc6
I0111 22:36:41.471145 32596 net.cpp:367] bn7 -> fc6 (in-place)
I0111 22:36:41.471452 32596 net.cpp:122] Setting up bn7
I0111 22:36:41.471464 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:41.471475 32596 net.cpp:137] Memory required for data: 511847200
I0111 22:36:41.471513 32596 layer_factory.hpp:78] Creating layer scale7
I0111 22:36:41.471551 32596 net.cpp:84] Creating Layer scale7
I0111 22:36:41.471560 32596 net.cpp:406] scale7 <- fc6
I0111 22:36:41.471575 32596 net.cpp:367] scale7 -> fc6 (in-place)
I0111 22:36:41.471696 32596 layer_factory.hpp:78] Creating layer scale7
I0111 22:36:41.471904 32596 net.cpp:122] Setting up scale7
I0111 22:36:41.471916 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:41.471920 32596 net.cpp:137] Memory required for data: 512666400
I0111 22:36:41.471935 32596 layer_factory.hpp:78] Creating layer binactive6
I0111 22:36:41.471966 32596 net.cpp:84] Creating Layer binactive6
I0111 22:36:41.471972 32596 net.cpp:406] binactive6 <- fc6
I0111 22:36:41.471998 32596 net.cpp:380] binactive6 -> binactive6
I0111 22:36:41.472061 32596 net.cpp:122] Setting up binactive6
I0111 22:36:41.472075 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:41.472079 32596 net.cpp:137] Memory required for data: 513485600
I0111 22:36:41.472085 32596 layer_factory.hpp:78] Creating layer fc7
I0111 22:36:41.472117 32596 net.cpp:84] Creating Layer fc7
I0111 22:36:41.472126 32596 net.cpp:406] fc7 <- binactive6
I0111 22:36:41.472144 32596 net.cpp:380] fc7 -> fc7
I0111 22:36:42.965476 32596 net.cpp:122] Setting up fc7
I0111 22:36:42.965526 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:42.965531 32596 net.cpp:137] Memory required for data: 514304800
I0111 22:36:42.965567 32596 layer_factory.hpp:78] Creating layer bn8
I0111 22:36:42.965606 32596 net.cpp:84] Creating Layer bn8
I0111 22:36:42.965625 32596 net.cpp:406] bn8 <- fc7
I0111 22:36:42.965661 32596 net.cpp:367] bn8 -> fc7 (in-place)
I0111 22:36:42.965963 32596 net.cpp:122] Setting up bn8
I0111 22:36:42.965977 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:42.965982 32596 net.cpp:137] Memory required for data: 515124000
I0111 22:36:42.966013 32596 layer_factory.hpp:78] Creating layer scale8
I0111 22:36:42.966043 32596 net.cpp:84] Creating Layer scale8
I0111 22:36:42.966053 32596 net.cpp:406] scale8 <- fc7
I0111 22:36:42.966096 32596 net.cpp:367] scale8 -> fc7 (in-place)
I0111 22:36:42.966214 32596 layer_factory.hpp:78] Creating layer scale8
I0111 22:36:42.966431 32596 net.cpp:122] Setting up scale8
I0111 22:36:42.966447 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:42.966454 32596 net.cpp:137] Memory required for data: 515943200
I0111 22:36:42.966475 32596 layer_factory.hpp:78] Creating layer relu7
I0111 22:36:42.966496 32596 net.cpp:84] Creating Layer relu7
I0111 22:36:42.966506 32596 net.cpp:406] relu7 <- fc7
I0111 22:36:42.966528 32596 net.cpp:367] relu7 -> fc7 (in-place)
I0111 22:36:42.967854 32596 net.cpp:122] Setting up relu7
I0111 22:36:42.967872 32596 net.cpp:129] Top shape: 50 4096 (204800)
I0111 22:36:42.967878 32596 net.cpp:137] Memory required for data: 516762400
I0111 22:36:42.967888 32596 layer_factory.hpp:78] Creating layer fc8
I0111 22:36:42.967922 32596 net.cpp:84] Creating Layer fc8
I0111 22:36:42.967933 32596 net.cpp:406] fc8 <- fc7
I0111 22:36:42.967965 32596 net.cpp:380] fc8 -> fc8
I0111 22:36:43.339251 32596 net.cpp:122] Setting up fc8
I0111 22:36:43.339308 32596 net.cpp:129] Top shape: 50 1000 (50000)
I0111 22:36:43.339313 32596 net.cpp:137] Memory required for data: 516962400
I0111 22:36:43.339362 32596 layer_factory.hpp:78] Creating layer fc8_fc8_0_split
I0111 22:36:43.339455 32596 net.cpp:84] Creating Layer fc8_fc8_0_split
I0111 22:36:43.339484 32596 net.cpp:406] fc8_fc8_0_split <- fc8
I0111 22:36:43.339520 32596 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0111 22:36:43.339551 32596 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0111 22:36:43.339566 32596 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_2
I0111 22:36:43.339680 32596 net.cpp:122] Setting up fc8_fc8_0_split
I0111 22:36:43.339717 32596 net.cpp:129] Top shape: 50 1000 (50000)
I0111 22:36:43.339735 32596 net.cpp:129] Top shape: 50 1000 (50000)
I0111 22:36:43.339740 32596 net.cpp:129] Top shape: 50 1000 (50000)
I0111 22:36:43.339745 32596 net.cpp:137] Memory required for data: 517562400
I0111 22:36:43.339752 32596 layer_factory.hpp:78] Creating layer loss
I0111 22:36:43.339769 32596 net.cpp:84] Creating Layer loss
I0111 22:36:43.339776 32596 net.cpp:406] loss <- fc8_fc8_0_split_0
I0111 22:36:43.339788 32596 net.cpp:406] loss <- label_data_1_split_0
I0111 22:36:43.339803 32596 net.cpp:380] loss -> loss
I0111 22:36:43.339828 32596 layer_factory.hpp:78] Creating layer loss
I0111 22:36:43.341004 32596 net.cpp:122] Setting up loss
I0111 22:36:43.341033 32596 net.cpp:129] Top shape: (1)
I0111 22:36:43.341040 32596 net.cpp:132]     with loss weight 1
I0111 22:36:43.341058 32596 net.cpp:137] Memory required for data: 517562404
I0111 22:36:43.341070 32596 layer_factory.hpp:78] Creating layer accuracy
I0111 22:36:43.341097 32596 net.cpp:84] Creating Layer accuracy
I0111 22:36:43.341111 32596 net.cpp:406] accuracy <- fc8_fc8_0_split_1
I0111 22:36:43.341135 32596 net.cpp:406] accuracy <- label_data_1_split_1
I0111 22:36:43.341161 32596 net.cpp:380] accuracy -> accuracy
I0111 22:36:43.341202 32596 net.cpp:122] Setting up accuracy
I0111 22:36:43.341219 32596 net.cpp:129] Top shape: (1)
I0111 22:36:43.341226 32596 net.cpp:137] Memory required for data: 517562408
I0111 22:36:43.341235 32596 layer_factory.hpp:78] Creating layer accuracy_5
I0111 22:36:43.341259 32596 net.cpp:84] Creating Layer accuracy_5
I0111 22:36:43.341270 32596 net.cpp:406] accuracy_5 <- fc8_fc8_0_split_2
I0111 22:36:43.341290 32596 net.cpp:406] accuracy_5 <- label_data_1_split_2
I0111 22:36:43.341310 32596 net.cpp:380] accuracy_5 -> accuracy_5
I0111 22:36:43.341351 32596 net.cpp:122] Setting up accuracy_5
I0111 22:36:43.341367 32596 net.cpp:129] Top shape: (1)
I0111 22:36:43.341375 32596 net.cpp:137] Memory required for data: 517562412
I0111 22:36:43.341390 32596 net.cpp:200] accuracy_5 does not need backward computation.
I0111 22:36:43.341403 32596 net.cpp:200] accuracy does not need backward computation.
I0111 22:36:43.341413 32596 net.cpp:198] loss needs backward computation.
I0111 22:36:43.341423 32596 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0111 22:36:43.341431 32596 net.cpp:198] fc8 needs backward computation.
I0111 22:36:43.341440 32596 net.cpp:198] relu7 needs backward computation.
I0111 22:36:43.341449 32596 net.cpp:198] scale8 needs backward computation.
I0111 22:36:43.341457 32596 net.cpp:198] bn8 needs backward computation.
I0111 22:36:43.341465 32596 net.cpp:198] fc7 needs backward computation.
I0111 22:36:43.341473 32596 net.cpp:198] binactive6 needs backward computation.
I0111 22:36:43.341483 32596 net.cpp:198] scale7 needs backward computation.
I0111 22:36:43.341491 32596 net.cpp:198] bn7 needs backward computation.
I0111 22:36:43.341500 32596 net.cpp:198] fc6 needs backward computation.
I0111 22:36:43.341508 32596 net.cpp:198] binactive5 needs backward computation.
I0111 22:36:43.341518 32596 net.cpp:198] scale6 needs backward computation.
I0111 22:36:43.341526 32596 net.cpp:198] bn6 needs backward computation.
I0111 22:36:43.341533 32596 net.cpp:198] pool5 needs backward computation.
I0111 22:36:43.341542 32596 net.cpp:198] conv5 needs backward computation.
I0111 22:36:43.341552 32596 net.cpp:198] binactive4 needs backward computation.
I0111 22:36:43.341560 32596 net.cpp:198] scale5 needs backward computation.
I0111 22:36:43.341572 32596 net.cpp:198] bn5 needs backward computation.
I0111 22:36:43.341583 32596 net.cpp:198] conv4 needs backward computation.
I0111 22:36:43.341593 32596 net.cpp:198] binactive3 needs backward computation.
I0111 22:36:43.341601 32596 net.cpp:198] scale4 needs backward computation.
I0111 22:36:43.341609 32596 net.cpp:198] bn4 needs backward computation.
I0111 22:36:43.341617 32596 net.cpp:198] conv3 needs backward computation.
I0111 22:36:43.341625 32596 net.cpp:198] binactive2 needs backward computation.
I0111 22:36:43.341657 32596 net.cpp:198] scale3 needs backward computation.
I0111 22:36:43.341666 32596 net.cpp:198] bn3 needs backward computation.
I0111 22:36:43.341676 32596 net.cpp:198] pool2 needs backward computation.
I0111 22:36:43.341686 32596 net.cpp:198] conv2 needs backward computation.
I0111 22:36:43.341696 32596 net.cpp:198] binactive1 needs backward computation.
I0111 22:36:43.341704 32596 net.cpp:198] scale2 needs backward computation.
I0111 22:36:43.341714 32596 net.cpp:198] bn2 needs backward computation.
I0111 22:36:43.341722 32596 net.cpp:198] pool1 needs backward computation.
I0111 22:36:43.341732 32596 net.cpp:198] relu1 needs backward computation.
I0111 22:36:43.341740 32596 net.cpp:198] scale1 needs backward computation.
I0111 22:36:43.341750 32596 net.cpp:198] bn1 needs backward computation.
I0111 22:36:43.341759 32596 net.cpp:198] conv1 needs backward computation.
I0111 22:36:43.341773 32596 net.cpp:200] label_data_1_split does not need backward computation.
I0111 22:36:43.341784 32596 net.cpp:200] data does not need backward computation.
I0111 22:36:43.341791 32596 net.cpp:242] This network produces output accuracy
I0111 22:36:43.341805 32596 net.cpp:242] This network produces output accuracy_5
I0111 22:36:43.341815 32596 net.cpp:242] This network produces output loss
I0111 22:36:43.341915 32596 net.cpp:255] Network initialization done.
I0111 22:36:43.342152 32596 solver.cpp:57] Solver scaffolding done.
I0111 22:36:43.347754 32596 caffe.cpp:239] Starting Optimization
I0111 22:36:43.347779 32596 solver.cpp:299] Solving AlexNet-BN
I0111 22:36:43.347786 32596 solver.cpp:300] Learning Rate Policy: modified_lr
I0111 22:36:43.353689 32596 solver.cpp:384] Iteration 0, Testing net (#0)
I0111 22:36:43.671171 32596 blocking_queue.cpp:49] Waiting for data
I0111 22:38:42.915642 32722 data_layer.cpp:73] Restarting data prefetching from start.
I0111 22:38:42.965173 32596 solver.cpp:452]     Test net output #0: accuracy = 0.00082
I0111 22:38:42.965246 32596 solver.cpp:452]     Test net output #1: accuracy_5 = 0.0048
I0111 22:38:42.965273 32596 solver.cpp:452]     Test net output #2: loss = 87.2232 (* 1 = 87.2232 loss)
I0111 22:38:42.965296 32596 solver.cpp:463] ================================
I0111 22:38:42.965359 32596 solver.cpp:464]     Test net best accuracy1 is: 0.00082
I0111 22:38:42.965402 32596 solver.cpp:466]     Test net best accuracy5 is: 0.0048
I0111 22:38:43.651767 32596 solver.cpp:242] Iteration 0 (1.16667e+18 iter/s, 120.299s/200 iters), loss = 7.33914
I0111 22:38:43.651834 32596 solver.cpp:261]     Train net output #0: accuracy = 0.00390625
I0111 22:38:43.651844 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.0078125
I0111 22:38:43.651867 32596 solver.cpp:261]     Train net output #2: loss = 7.33914 (* 1 = 7.33914 loss)
I0111 22:38:43.651907 32596 sgd_solver.cpp:122] Iteration 0, lr = 0.001
I0111 22:39:05.846462 32596 blocking_queue.cpp:49] Waiting for data
I0111 22:41:07.120633 32596 solver.cpp:242] Iteration 200 (1.39408 iter/s, 143.463s/200 iters), loss = 7.0894
I0111 22:41:07.132551 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0117188
I0111 22:41:07.132664 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.015625
I0111 22:41:07.132728 32596 solver.cpp:261]     Train net output #2: loss = 7.0894 (* 1 = 7.0894 loss)
I0111 22:41:07.132742 32596 sgd_solver.cpp:122] Iteration 200, lr = 0.001
I0111 22:43:33.055060 32596 solver.cpp:242] Iteration 400 (1.37064 iter/s, 145.917s/200 iters), loss = 6.58532
I0111 22:43:33.067490 32596 solver.cpp:261]     Train net output #0: accuracy = 0.03125
I0111 22:43:33.067569 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.0703125
I0111 22:43:33.067610 32596 solver.cpp:261]     Train net output #2: loss = 6.58532 (* 1 = 6.58532 loss)
I0111 22:43:33.067636 32596 sgd_solver.cpp:122] Iteration 400, lr = 0.001
I0111 22:45:57.418413 32596 solver.cpp:242] Iteration 600 (1.38556 iter/s, 144.346s/200 iters), loss = 6.10897
I0111 22:45:57.418553 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0351562
I0111 22:45:57.418599 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.109375
I0111 22:45:57.418658 32596 solver.cpp:261]     Train net output #2: loss = 6.10897 (* 1 = 6.10897 loss)
I0111 22:45:57.418678 32596 sgd_solver.cpp:122] Iteration 600, lr = 0.001
I0111 22:48:22.085172 32596 solver.cpp:242] Iteration 800 (1.38254 iter/s, 144.661s/200 iters), loss = 5.9206
I0111 22:48:22.085335 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0351562
I0111 22:48:22.085361 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.125
I0111 22:48:22.085400 32596 solver.cpp:261]     Train net output #2: loss = 5.9206 (* 1 = 5.9206 loss)
I0111 22:48:22.085418 32596 sgd_solver.cpp:122] Iteration 800, lr = 0.001
I0111 22:50:46.186679 32596 solver.cpp:384] Iteration 1000, Testing net (#0)
I0111 22:50:50.543191 32596 blocking_queue.cpp:49] Waiting for data
I0111 22:52:43.650651 32722 data_layer.cpp:73] Restarting data prefetching from start.
I0111 22:52:43.700822 32596 solver.cpp:452]     Test net output #0: accuracy = 0.0280602
I0111 22:52:43.700882 32596 solver.cpp:452]     Test net output #1: accuracy_5 = 0.0899801
I0111 22:52:43.700897 32596 solver.cpp:452]     Test net output #2: loss = 6.79094 (* 1 = 6.79094 loss)
I0111 22:52:43.700909 32596 solver.cpp:463] ================================
I0111 22:52:43.700914 32596 solver.cpp:464]     Test net best accuracy1 is: 0.0280602
I0111 22:52:43.700922 32596 solver.cpp:466]     Test net best accuracy5 is: 0.0899801
I0111 22:52:44.379673 32596 solver.cpp:242] Iteration 1000 (0.76253 iter/s, 262.285s/200 iters), loss = 5.64037
I0111 22:52:44.382038 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0742188
I0111 22:52:44.382062 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.15625
I0111 22:52:44.382086 32596 solver.cpp:261]     Train net output #2: loss = 5.64037 (* 1 = 5.64037 loss)
I0111 22:52:44.382115 32596 sgd_solver.cpp:122] Iteration 1000, lr = 0.001
I0111 22:53:27.736995 32596 blocking_queue.cpp:49] Waiting for data
I0111 22:55:08.756098 32596 solver.cpp:242] Iteration 1200 (1.38534 iter/s, 144.369s/200 iters), loss = 5.37717
I0111 22:55:08.768265 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0742188
I0111 22:55:08.768332 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.175781
I0111 22:55:08.768371 32596 solver.cpp:261]     Train net output #2: loss = 5.37717 (* 1 = 5.37717 loss)
I0111 22:55:08.768385 32596 sgd_solver.cpp:122] Iteration 1200, lr = 0.001
I0111 22:57:33.760421 32596 solver.cpp:242] Iteration 1400 (1.37944 iter/s, 144.987s/200 iters), loss = 5.23684
I0111 22:57:33.772286 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0664062
I0111 22:57:33.772312 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.199219
I0111 22:57:33.772337 32596 solver.cpp:261]     Train net output #2: loss = 5.23684 (* 1 = 5.23684 loss)
I0111 22:57:33.772372 32596 sgd_solver.cpp:122] Iteration 1400, lr = 0.001
I0111 22:59:59.579457 32596 solver.cpp:242] Iteration 1600 (1.37173 iter/s, 145.802s/200 iters), loss = 5.27449
I0111 22:59:59.591156 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0625
I0111 22:59:59.591213 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.167969
I0111 22:59:59.591244 32596 solver.cpp:261]     Train net output #2: loss = 5.27449 (* 1 = 5.27449 loss)
I0111 22:59:59.591259 32596 sgd_solver.cpp:122] Iteration 1600, lr = 0.001
I0111 23:02:33.130198 32596 solver.cpp:242] Iteration 1800 (1.30264 iter/s, 153.534s/200 iters), loss = 5.15886
I0111 23:02:33.142257 32596 solver.cpp:261]     Train net output #0: accuracy = 0.0820312
I0111 23:02:33.142280 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.226562
I0111 23:02:33.142323 32596 solver.cpp:261]     Train net output #2: loss = 5.15886 (* 1 = 5.15886 loss)
I0111 23:02:33.142334 32596 sgd_solver.cpp:122] Iteration 1800, lr = 0.001
I0111 23:05:04.361840 32596 solver.cpp:384] Iteration 2000, Testing net (#0)
I0111 23:05:13.020236 32596 blocking_queue.cpp:49] Waiting for data
I0111 23:07:03.549950 32722 data_layer.cpp:73] Restarting data prefetching from start.
I0111 23:07:03.600909 32596 solver.cpp:452]     Test net output #0: accuracy = 0.0547003
I0111 23:07:03.600982 32596 solver.cpp:452]     Test net output #1: accuracy_5 = 0.15472
I0111 23:07:03.601006 32596 solver.cpp:452]     Test net output #2: loss = 6.09914 (* 1 = 6.09914 loss)
I0111 23:07:03.601018 32596 solver.cpp:463] ================================
I0111 23:07:03.601024 32596 solver.cpp:464]     Test net best accuracy1 is: 0.0547003
I0111 23:07:03.601032 32596 solver.cpp:466]     Test net best accuracy5 is: 0.15472
I0111 23:07:04.281532 32596 solver.cpp:242] Iteration 2000 (0.737653 iter/s, 271.13s/200 iters), loss = 4.97126
I0111 23:07:04.283844 32596 solver.cpp:261]     Train net output #0: accuracy = 0.121094
I0111 23:07:04.283871 32596 solver.cpp:261]     Train net output #1: accuracy_5 = 0.253906
I0111 23:07:04.283900 32596 solver.cpp:261]     Train net output #2: loss = 4.97126 (* 1 = 4.97126 loss)
I0111 23:07:04.283921 32596 sgd_solver.cpp:122] Iteration 2000, lr = 0.001
I0111 23:07:59.444195 32596 blocking_queue.cpp:49] Waiting for data
